# Checkpoint Compatibility Analysis: train_pytorch.py vs tracin_optimized.py

## Executive Summary

‚ö†Ô∏è **COMPATIBILITY ISSUE IDENTIFIED**: The checkpoints saved by `train_pytorch.py` are **NOT directly compatible** with `tracin_optimized.py` due to different model architectures and state dict formats.

## Problem Overview

### Two Different ConvE Implementations

1. **train_pytorch.py**: Uses **custom pure PyTorch ConvE** (lines 67-188)
   - Direct PyTorch nn.Module implementation
   - Simple layer names: `entity_embeddings`, `conv1`, `fc`, etc.

2. **tracin_optimized.py**: Expects **PyKEEN ConvE model**
   - PyKEEN's ConvE class from `pykeen.models`
   - Complex layer names: `entity_representations.0._embeddings`, `interaction.hr2d.2`, etc.

## Detailed Comparison

### State Dict Format Differences

#### train_pytorch.py Checkpoint Format
```python
{
    'epoch': 50,
    'model_state_dict': {
        'entity_embeddings.weight': tensor(...),      # Custom name
        'relation_embeddings.weight': tensor(...),    # Custom name
        'conv1.weight': tensor(...),                  # Custom name
        'conv1.bias': tensor(...),
        'fc.weight': tensor(...),                     # Custom name
        'fc.bias': tensor(...),
        'bn0.weight': tensor(...),
        'bn1.weight': tensor(...),
        'bn2.weight': tensor(...),
        ...
    },
    'optimizer_state_dict': {...},
    'model_config': {
        'num_entities': 50000,
        'num_relations': 100,
        'embedding_dim': 200,
        'embedding_height': 10,
        'embedding_width': 20,
        'output_channels': 32
    }
}
```

#### PyKEEN ConvE State Dict Format
```python
{
    'entity_representations.0._embeddings.weight': tensor(...),  # PyKEEN name
    'relation_representations.0._embeddings.weight': tensor(...), # PyKEEN name
    'interaction.hr2d.0.weight': tensor(...),                     # BatchNorm
    'interaction.hr2d.2.weight': tensor(...),                     # Conv2d (PyKEEN name)
    'interaction.hr2d.2.bias': tensor(...),
    'interaction.hr2d.3.weight': tensor(...),                     # BatchNorm
    'interaction.hr1d.0.weight': tensor(...),                     # Linear (PyKEEN name)
    'interaction.hr1d.0.bias': tensor(...),
    'interaction.hr1d.1.weight': tensor(...),                     # BatchNorm
    ...
}
```

### Layer Name Mapping

| train_pytorch.py | PyKEEN ConvE | Description |
|-----------------|--------------|-------------|
| `entity_embeddings.weight` | `entity_representations.0._embeddings.weight` | Entity embeddings |
| `relation_embeddings.weight` | `relation_representations.0._embeddings.weight` | Relation embeddings |
| `conv1.weight` | `interaction.hr2d.2.weight` | Convolutional layer |
| `conv1.bias` | `interaction.hr2d.2.bias` | Conv bias |
| `fc.weight` | `interaction.hr1d.0.weight` | Final linear layer |
| `fc.bias` | `interaction.hr1d.0.bias` | Linear bias |
| `bn0.*` | `interaction.hr2d.0.*` | Input batch norm |
| `bn1.*` | `interaction.hr2d.3.*` | Conv batch norm |
| `bn2.*` | `interaction.hr1d.1.*` | Final batch norm |

## Root Cause

The issue stems from using two different implementations:

1. **train_pytorch.py** was created for:
   - **Memory-efficient training** with custom loss computation
   - Full control over training loop for checkpointing
   - Simplicity and transparency

2. **tracin_optimized.py** was designed for:
   - **PyKEEN models** with its specific architecture
   - Vectorized gradient computation using PyKEEN's interface
   - Compatible with existing PyKEEN-trained models

## Impact on TracIn

### Current Situation

‚ùå **Cannot use train_pytorch.py checkpoints directly with tracin_optimized.py**

The incompatibility means:
- `run_tracin.py` cannot load train_pytorch.py checkpoints
- Trying to load will result in state dict key mismatch errors
- No vectorized TracIn speedup available for train_pytorch.py models

### Why This Matters

The memory-efficient training in train_pytorch.py allows:
- ‚úÖ Batch sizes of 512-1024 (vs 256)
- ‚úÖ 50,000x memory reduction during training
- ‚úÖ Faster training with large batches

But TracIn needs PyKEEN format to get:
- ‚úÖ Vectorized gradient computation (10-20x speedup)
- ‚úÖ Test gradient caching
- ‚úÖ torch.compile optimizations
- ‚úÖ Multi-GPU support

## Solutions

### Option 1: Create State Dict Converter (RECOMMENDED)

Create a converter function that translates between formats:

```python
def convert_pytorch_to_pykeen_state_dict(pytorch_checkpoint):
    """Convert train_pytorch.py checkpoint to PyKEEN format."""

    pytorch_state = pytorch_checkpoint['model_state_dict']
    pykeen_state = {}

    # Map layer names
    name_map = {
        'entity_embeddings.weight': 'entity_representations.0._embeddings.weight',
        'relation_embeddings.weight': 'relation_representations.0._embeddings.weight',
        'conv1.weight': 'interaction.hr2d.2.weight',
        'conv1.bias': 'interaction.hr2d.2.bias',
        'fc.weight': 'interaction.hr1d.0.weight',
        'fc.bias': 'interaction.hr1d.0.bias',
        'bn0.weight': 'interaction.hr2d.0.weight',
        'bn0.bias': 'interaction.hr2d.0.bias',
        'bn0.running_mean': 'interaction.hr2d.0.running_mean',
        'bn0.running_var': 'interaction.hr2d.0.running_var',
        'bn0.num_batches_tracked': 'interaction.hr2d.0.num_batches_tracked',
        'bn1.weight': 'interaction.hr2d.3.weight',
        'bn1.bias': 'interaction.hr2d.3.bias',
        'bn1.running_mean': 'interaction.hr2d.3.running_mean',
        'bn1.running_var': 'interaction.hr2d.3.running_var',
        'bn1.num_batches_tracked': 'interaction.hr2d.3.num_batches_tracked',
        'bn2.weight': 'interaction.hr1d.1.weight',
        'bn2.bias': 'interaction.hr1d.1.bias',
        'bn2.running_mean': 'interaction.hr1d.1.running_mean',
        'bn2.running_var': 'interaction.hr1d.1.running_var',
        'bn2.num_batches_tracked': 'interaction.hr1d.1.num_batches_tracked',
    }

    for old_name, new_name in name_map.items():
        if old_name in pytorch_state:
            pykeen_state[new_name] = pytorch_state[old_name]

    return pykeen_state
```

**Pros:**
- ‚úÖ Keeps both implementations
- ‚úÖ Enables all TracIn optimizations
- ‚úÖ Backward compatible
- ‚úÖ One-time conversion per checkpoint

**Cons:**
- ‚ö†Ô∏è Requires maintaining converter
- ‚ö†Ô∏è Needs thorough testing

### Option 2: Create Custom TracIn for PyTorch Models

Adapt `tracin_optimized.py` to work with custom PyTorch models:

```python
class TracInAnalyzerPyTorch(TracInAnalyzer):
    """TracIn analyzer for custom PyTorch ConvE models."""

    def __init__(self, pytorch_model, ...):
        # Adapt to work with custom PyTorch model
        # Implement score_t() method wrapper
        ...
```

**Pros:**
- ‚úÖ Direct compatibility
- ‚úÖ No conversion needed
- ‚úÖ Can optimize for custom model

**Cons:**
- ‚ö†Ô∏è Duplicate code maintenance
- ‚ö†Ô∏è May not support all PyKEEN-specific optimizations

### Option 3: Use PyKEEN for Training (FALLBACK)

Switch back to using PyKEEN's ConvE with custom loss function:

```python
from pykeen.models import ConvE
from pykeen.training import SLCWATrainingLoop

# Use PyKEEN model but inject efficient loss computation
class EfficientBCELoss(nn.Module):
    def forward(self, scores, tail):
        return compute_loss_efficient(scores, tail, label_smoothing)
```

**Pros:**
- ‚úÖ Full PyKEEN compatibility
- ‚úÖ All TracIn optimizations work
- ‚úÖ No conversion needed

**Cons:**
- ‚ö†Ô∏è Less control over training loop
- ‚ö†Ô∏è May not be able to fully integrate efficient loss
- ‚ö†Ô∏è Loses transparency of custom implementation

## Recommended Approach

### Phase 1: Immediate Solution (Converter)

1. **Create converter script** (`convert_checkpoint.py`):
   ```bash
   python convert_checkpoint.py \
       --input models/conve/checkpoints/checkpoint_epoch_50.pt \
       --output models/conve/checkpoints/checkpoint_epoch_50_pykeen.pt
   ```

2. **Update run_tracin.py** to auto-detect and convert:
   ```python
   if checkpoint_format == 'pytorch':
       checkpoint = convert_pytorch_to_pykeen_checkpoint(checkpoint)
   ```

3. **Test thoroughly**:
   - Verify state dict shapes match
   - Compare model outputs
   - Validate TracIn results

### Phase 2: Long-term Solution (Unified Training)

Consider creating a hybrid approach:
- Use PyKEEN's model architecture
- Inject memory-efficient loss via custom training loop
- Best of both worlds: compatibility + efficiency

## Verification Steps

To verify checkpoint compatibility:

```python
# 1. Load train_pytorch.py checkpoint
pytorch_ckpt = torch.load('checkpoint_epoch_50.pt')
pytorch_state = pytorch_ckpt['model_state_dict']

# 2. Load PyKEEN model
from pykeen.models import ConvE
pykeen_model = ConvE(...)

# 3. Check layer name differences
pytorch_keys = set(pytorch_state.keys())
pykeen_keys = set(pykeen_model.state_dict().keys())

print("Only in PyTorch:", pytorch_keys - pykeen_keys)
print("Only in PyKEEN:", pykeen_keys - pytorch_keys)

# 4. Check tensor shapes
for key in pytorch_keys & pykeen_keys:
    if pytorch_state[key].shape != pykeen_model.state_dict()[key].shape:
        print(f"Shape mismatch for {key}")
```

## Implementation Priority

### High Priority (Immediate)
1. ‚úÖ Create state dict converter function
2. ‚úÖ Add auto-detection to run_tracin.py
3. ‚úÖ Test on sample checkpoints

### Medium Priority (Short-term)
4. ‚ö†Ô∏è Create comprehensive test suite
5. ‚ö†Ô∏è Document conversion process
6. ‚ö†Ô∏è Add conversion to pipeline

### Low Priority (Long-term)
7. üìã Evaluate unified training approach
8. üìã Consider PyTorch-native TracIn implementation
9. üìã Benchmark performance differences

## Testing Checklist

Before deploying the converter:

- [ ] Verify all layer names mapped correctly
- [ ] Check tensor shapes match exactly
- [ ] Compare model outputs on same input
- [ ] Validate TracIn scores match (if possible)
- [ ] Test with multiple checkpoint epochs
- [ ] Verify batch norm statistics preserved
- [ ] Check optimizer state conversion (if needed)
- [ ] Test on different model configurations
- [ ] Benchmark TracIn speedup
- [ ] Document any known limitations

## Conclusion

The checkpoint format incompatibility is a **solvable issue** that requires creating a state dict converter. The recommended approach is:

1. **Short-term**: Create and test converter function
2. **Medium-term**: Integrate into pipeline
3. **Long-term**: Evaluate unified training approach

The memory-efficient training from train_pytorch.py is **valuable** and should be preserved. The TracIn optimizations from tracin_optimized.py are also **valuable**. The converter bridges these two implementations until a unified solution is developed.

## Next Steps

1. Implement `convert_checkpoint.py` script
2. Test conversion on sample checkpoints
3. Update `run_tracin.py` to auto-detect format
4. Validate TracIn results
5. Document usage in pipeline

---

**Status**: Analysis Complete ‚úÖ
**Compatibility**: Not Compatible (requires converter) ‚ö†Ô∏è
**Solution**: Create state dict converter üîß
**Priority**: High (blocks TracIn on new checkpoints) üö®
