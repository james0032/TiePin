"""
Snakemake pipeline for ConvE PyTorch Knowledge Graph Embedding and TracIn Analysis

This pipeline uses the memory-optimized train_pytorch.py implementation that supports
larger batch sizes compared to naive approaches through efficient loss computation.

Key Features:
- Memory-efficient training with 10,000-50,000x reduction in label storage
- Support for large batch sizes (512-1024+) for faster training
- Mixed precision (FP16) training for 2x memory and speed improvements
- Full checkpoint support for TracIn analysis
- Compatible with all TracIn tools (run_tracin.py, batch_tracin_with_filtering.py)

This pipeline automates the complete workflow:
1. Create ROBOKOP subgraph (with style-based filtering)
1b. Prepare dictionary files
2. Extract mechanistic paths from DrugMechDB
2b. Filter treats edges with DrugMechDB paths
3. Extract DrugMechDB test set
4. Split data into train/valid
5. Train ConvE model (using optimized train_pytorch.py)
6. Evaluate model
7. TracIn analysis (checkpoints available in models/conve/checkpoints/)

Usage:
    snakemake --snakefile Snakefile_pytorch_conve --cores all
    snakemake --snakefile Snakefile_pytorch_conve --cores all --configfile config.yaml
    snakemake --snakefile Snakefile_pytorch_conve -n  # Dry run
"""

configfile: "config.yaml"

# Get style from config to use as base directory
# All outputs will be organized under {BASE_DIR}/{style}/
STYLE = config.get("style", "CGGD_alltreat")
BASE_DIR = f"{config['BASE_DIR']}/{STYLE}"

# Define all final outputs
rule all:
    input:
        # Subgraph files
        f"{BASE_DIR}/rotorobo.txt",
        f"{BASE_DIR}/edge_map.json",
        # Mechanistic paths output
        f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_path_id_results.txt",
        # DrugMechDB test set
        f"{BASE_DIR}/test.txt",
        f"{BASE_DIR}/train_candidates.txt",
        # Train/valid split
        f"{BASE_DIR}/train.txt",
        f"{BASE_DIR}/valid.txt",
        # Dictionary files
        f"{BASE_DIR}/processed/node_dict.txt",
        f"{BASE_DIR}/processed/rel_dict.txt",
        # Trained model (train_pytorch.py outputs)
        f"{BASE_DIR}/models/conve/config.json",
        f"{BASE_DIR}/models/conve/final_model.pt",
        # Evaluation results
        f"{BASE_DIR}/results/evaluation/test_scores.json",
        f"{BASE_DIR}/results/evaluation/test_scores_ranked.json"
        # Note: TracIn checkpoints are in models/conve/checkpoints/

# ============================================================================
# Step 1: Create ROBOKOP Subgraph
# ============================================================================

rule create_subgraph:
    """
    Create ROBOKOP subgraph from edges file with specified filtering style
    The output directory is automatically robokop/{style}/ based on --style parameter
    """
    input:
        node_file = config["node_file"],
        edges_file = config["edges_file"]
    output:
        subgraph = f"{BASE_DIR}/rotorobo.txt",
        edge_map = f"{BASE_DIR}/edge_map.json"
    params:
        style = STYLE,
        outdir = BASE_DIR
    log:
        f"{BASE_DIR}/logs/create_subgraph.log"
    shell:
        """
        python src/create_robokop_subgraph.py \
            --node-file {input.node_file} \
            --edges-file {input.edges_file} \
            --style {params.style} \
            --outdir {params.outdir} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 1b: Prepare Dictionary Files
# ============================================================================

rule prepare_dictionaries:
    """
    Generate node_dict and rel_dict from subgraph data
    """
    input:
        subgraph = f"{BASE_DIR}/rotorobo.txt",
        edge_map = f"{BASE_DIR}/edge_map.json",
        nodes_file = config["node_file"]
    output:
        node_dict = f"{BASE_DIR}/processed/node_dict.txt",
        node_name_dict = f"{BASE_DIR}/processed/node_name_dict.txt",
        rel_dict = f"{BASE_DIR}/processed/rel_dict.txt",
        stats = f"{BASE_DIR}/processed/graph_stats.txt"
    params:
        output_dir = f"{BASE_DIR}/processed"
    log:
        f"{BASE_DIR}/logs/prepare_dictionaries.log"
    shell:
        """
        python src/prepare_dict.py \
            --input {input.subgraph} \
            --edge-map {input.edge_map} \
            --nodes-file {input.nodes_file} \
            --output-dir {params.output_dir} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 2: Extract Mechanistic Paths from DrugMechDB
# ============================================================================

rule extract_mechanistic_paths:
    """
    Extract drug-disease mechanistic paths from DrugMechDB using path_id
    Note: Depends on Step 1 completing first to ensure sequential execution
    """
    input:
        edges_file = config["edges_file"],
        subgraph = f"{BASE_DIR}/rotorobo.txt"  # Wait for Step 1 to complete
    output:
        path_results = f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_path_id_results.txt",
        treats_tsv = f"{BASE_DIR}/results/mechanistic_paths/treats.txt",
        json_results = f"{BASE_DIR}/results/mechanistic_paths/treats_mechanistic_paths.json" if config.get("run_old_method", False) else []
    params:
        output_dir = f"{BASE_DIR}/results/mechanistic_paths",
        max_length = config.get("max_path_length", 5),
        run_old = "--run_old_method" if config.get("run_old_method", False) else ""
    log:
        f"{BASE_DIR}/logs/extract_mechanistic_paths.log"
    shell:
        """
        python src/find_mechanistic_paths.py \
            --edges_file {input.edges_file} \
            --max_length {params.max_length} \
            --output_dir {params.output_dir} \
            {params.run_old} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 2b: Filter Treats Edges with DrugMechDB Paths
# ============================================================================

rule filter_treats_with_drugmechdb:
    """
    Filter treats edges to only include those with DrugMechDB mechanistic paths
    Uses add_pair_exists_column.py to cross-reference treats.txt with drugmechdb_path_id_results.txt
    """
    input:
        path_results_txt = f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_path_id_results.txt",
        treats_tsv = f"{BASE_DIR}/results/mechanistic_paths/treats.txt"
    output:
        filtered_tsv = f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_treats_filtered.txt",
        annotated_csv = f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_path_id_results_annotated.csv",
        annotated_tsv = f"{BASE_DIR}/results/mechanistic_paths/treats_annotated.txt"
    log:
        f"{BASE_DIR}/logs/filter_treats_with_drugmechdb.log"
    shell:
        """
        python utils/add_pair_exists_column.py \
            {input.path_results_txt} \
            {input.treats_tsv} \
            {output.annotated_csv} \
            {output.annotated_tsv} \
            {output.filtered_tsv} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 3: Extract DrugMechDB Test Set
# ============================================================================

rule extract_drugmechdb_test:
    """
    Extract test edges from DrugMechDB-verified treats edges and remove them from rotorobo.txt
    Uses edge_map.json to convert biolink:treats to predicate IDs for matching
    """
    input:
        subgraph = f"{BASE_DIR}/rotorobo.txt",
        edge_map = f"{BASE_DIR}/edge_map.json",  # Required: maps biolink:treats to predicate IDs
        filtered_tsv = f"{BASE_DIR}/results/mechanistic_paths/drugmechdb_treats_filtered.txt"
    output:
        test = f"{BASE_DIR}/test.txt",
        train_candidates = f"{BASE_DIR}/train_candidates.txt",
        stats = f"{BASE_DIR}/test_statistics.json"
    params:
        input_dir = BASE_DIR,
        test_pct = config.get("drugmechdb_test_pct", 0.10),
        seed = config.get("random_seed", 42)
    log:
        f"{BASE_DIR}/logs/extract_drugmechdb_test.log"
    shell:
        """
        python src/make_test_with_drugmechdb_treat.py \
            --input-dir {params.input_dir} \
            --filtered-tsv {input.filtered_tsv} \
            --test-pct {params.test_pct} \
            --seed {params.seed} \
            --output-dir {params.input_dir} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 4: Split Data into Train/Valid
# ============================================================================

rule split_data:
    """
    Split train_candidates into training and validation sets
    Note: test.txt is already created in Step 3
    """
    input:
        train_candidates = f"{BASE_DIR}/train_candidates.txt"
    output:
        train = f"{BASE_DIR}/train.txt",
        valid = f"{BASE_DIR}/valid.txt",
        stats = f"{BASE_DIR}/split_statistics.json"
    params:
        output_dir = BASE_DIR,
        train_ratio = config.get("train_ratio", 0.9),
        seed = config.get("random_seed", 42)
    log:
        f"{BASE_DIR}/logs/split_data.log"
    shell:
        """
        python src/train_valid_split.py \
            --input {input.train_candidates} \
            --output-dir {params.output_dir} \
            --train-ratio {params.train_ratio} \
            --seed {params.seed} \
            2>&1 | tee {log}
        """

# ============================================================================
# Step 5: Train ConvE Model (Memory-Optimized PyTorch Implementation)
# ============================================================================

rule train_model:
    """
    Train ConvE knowledge graph embedding model using memory-optimized train_pytorch.py

    Key Features:
    - Memory-efficient loss computation (10,000-50,000x reduction in label storage)
    - Supports large batch sizes (512-1024+) for faster training
    - Mixed precision (FP16) training for 2x memory and speed boost
    - Checkpoint files saved for TracIn analysis

    Memory Optimizations:
    1. Efficient loss computation: Avoids creating [batch_size, num_entities] label matrix
       - Old: ~50MB per batch (batch=256, entities=50k)
       - New: ~1KB per batch (1000x reduction!)

    2. Mixed precision (--use-mixed-precision): 2x memory + 2x speed on modern GPUs

    3. Memory cleanup: Explicit tensor deletion and periodic CUDA cache clearing

    Recommended Settings:
    - Standard training: batch_size=512, use_mixed_precision=true
    - Large KG (50k+ entities): batch_size=1024, use_mixed_precision=true
    - Memory-constrained: batch_size=256, use_mixed_precision=true
    """
    input:
        train = f"{BASE_DIR}/train.txt",
        valid = f"{BASE_DIR}/valid.txt",
        test = f"{BASE_DIR}/test.txt",
        node_dict = f"{BASE_DIR}/processed/node_dict.txt",
        rel_dict = f"{BASE_DIR}/processed/rel_dict.txt"
    output:
        # train_pytorch.py outputs
        config_out = f"{BASE_DIR}/models/conve/config.json",
        final_model = f"{BASE_DIR}/models/conve/final_model.pt",
        best_model = f"{BASE_DIR}/models/conve/best_model.pt",
        training_history = f"{BASE_DIR}/models/conve/training_history.json",
        test_results = f"{BASE_DIR}/models/conve/test_results.json"
        # Checkpoint files are automatically saved in models/conve/checkpoints/
    params:
        output_dir = f"{BASE_DIR}/models/conve",
        num_epochs = config.get("num_epochs", 100),
        batch_size = config.get("batch_size", 512),  # Increased from 256 - now possible!
        learning_rate = config.get("learning_rate", 0.001),
        embedding_dim = config.get("embedding_dim", 200),
        embedding_height = config.get("embedding_height", 10),
        embedding_width = config.get("embedding_width", 20),
        output_channels = config.get("output_channels", 32),
        kernel_height = config.get("kernel_height", 3),
        kernel_width = config.get("kernel_width", 3),
        input_dropout = config.get("input_dropout", 0.2),
        feature_map_dropout = config.get("feature_map_dropout", 0.2),
        output_dropout = config.get("output_dropout", 0.3),
        label_smoothing = config.get("label_smoothing", 0.1),
        checkpoint_frequency = config.get("checkpoint_frequency", 2),
        random_seed = config.get("random_seed", 42),
        num_workers = config.get("num_workers", 4),
        # Memory optimization flags
        use_mixed_precision = "--use-mixed-precision" if config.get("use_mixed_precision", True) else "",
        disable_memory_cleanup = "--disable-memory-cleanup" if config.get("disable_memory_cleanup", False) else "",
        no_gpu = "--no-gpu" if not config.get("use_gpu", True) else ""
    log:
        f"{BASE_DIR}/logs/train_model.log"
    resources:
        gpu = 1 if config.get("use_gpu", True) else 0
    shell:
        """
        echo "==================================================================="
        echo "Training ConvE with Memory-Optimized Implementation"
        echo "==================================================================="
        echo "Batch size: {params.batch_size}"
        echo "Mixed precision: {params.use_mixed_precision}"
        echo "Epochs: {params.num_epochs}"
        echo "Output: {params.output_dir}"
        echo "==================================================================="

        python train_pytorch.py \
            --train {input.train} \
            --valid {input.valid} \
            --test {input.test} \
            --entity-to-id {input.node_dict} \
            --relation-to-id {input.rel_dict} \
            --output-dir {params.output_dir} \
            --num-epochs {params.num_epochs} \
            --batch-size {params.batch_size} \
            --learning-rate {params.learning_rate} \
            --embedding-dim {params.embedding_dim} \
            --embedding-height {params.embedding_height} \
            --embedding-width {params.embedding_width} \
            --output-channels {params.output_channels} \
            --kernel-height {params.kernel_height} \
            --kernel-width {params.kernel_width} \
            --input-dropout {params.input_dropout} \
            --feature-map-dropout {params.feature_map_dropout} \
            --output-dropout {params.output_dropout} \
            --label-smoothing {params.label_smoothing} \
            --checkpoint-frequency {params.checkpoint_frequency} \
            --random-seed {params.random_seed} \
            --num-workers {params.num_workers} \
            {params.use_mixed_precision} \
            {params.disable_memory_cleanup} \
            {params.no_gpu} \
            2>&1 | tee {log}

        echo "==================================================================="
        echo "Training complete! Checkpoints saved in:"
        echo "  {params.output_dir}/checkpoints/"
        echo "==================================================================="
        """

# ============================================================================
# Step 6: Evaluate Model (Score Test Triples)
# ============================================================================

rule evaluate_model:
    """
    Score test triples using trained ConvE model with score_only.py
    This runs AFTER train.py completes and provides detailed scoring

    Note: This uses the PyKEEN-compatible score_only.py script.
    For train_pytorch.py models, you may need to adapt the loading mechanism.
    """
    input:
        # Ensure training is complete
        config_out = f"{BASE_DIR}/models/conve/config.json",
        final_model = f"{BASE_DIR}/models/conve/final_model.pt",
        test = f"{BASE_DIR}/test.txt",
        node_dict = f"{BASE_DIR}/processed/node_dict.txt",
        node_name_dict = f"{BASE_DIR}/processed/node_name_dict.txt",
        rel_dict = f"{BASE_DIR}/processed/rel_dict.txt"
    output:
        scores_json = f"{BASE_DIR}/results/evaluation/test_scores.json",
        scores_csv = f"{BASE_DIR}/results/evaluation/test_scores.csv",
        scores_ranked_json = f"{BASE_DIR}/results/evaluation/test_scores_ranked.json",
        scores_ranked_csv = f"{BASE_DIR}/results/evaluation/test_scores_ranked.csv"
    params:
        model_dir = f"{BASE_DIR}/models/conve",
        output_dir = f"{BASE_DIR}/results/evaluation",
        use_sigmoid = "--use-sigmoid" if config.get("use_sigmoid", False) else "",
        top_n_arg = f"--top-n {config.get('top_n_triples')}" if config.get("top_n_triples") else "",
        device = "cuda" if config.get("use_gpu", True) else "cpu"
    log:
        f"{BASE_DIR}/logs/evaluate_model.log"
    shell:
        """
        mkdir -p {params.output_dir}

        echo "==================================================================="
        echo "Evaluating trained model on test set"
        echo "Model: {params.model_dir}"
        echo "Test triples: {input.test}"
        echo "==================================================================="

        python score_only.py \
            --model-dir {params.model_dir} \
            --test {input.test} \
            --entity-to-id {input.node_dict} \
            --relation-to-id {input.rel_dict} \
            --node-name-dict {input.node_name_dict} \
            --output {output.scores_json} \
            --device {params.device} \
            {params.use_sigmoid} \
            {params.top_n_arg} \
            2>&1 | tee {log}

        echo "==================================================================="
        echo "Evaluation complete! Results saved to:"
        echo "  {params.output_dir}/"
        echo "==================================================================="
        """

# ============================================================================
# Step 7: TracIn Analysis (READY TO USE)
# ============================================================================
# TracIn analysis is fully supported with train_pytorch.py checkpoints.
# Checkpoint files are automatically saved in {BASE_DIR}/models/conve/checkpoints/
#
# To run TracIn analysis:
#   python run_tracin.py --help
#   python batch_tracin_with_filtering.py --help
#
# Example TracIn commands:
#   # Basic TracIn on a single test triple
#   python run_tracin.py \
#       --test-triple "drug_id relation_id disease_id" \
#       --checkpoint-dir robokop/CGGD_alltreat/models/conve/checkpoints \
#       --train-file robokop/CGGD_alltreat/train.txt \
#       --output tracin_results.json
#
#   # Batch TracIn with proximity filtering
#   python batch_tracin_with_filtering.py \
#       --test-file robokop/CGGD_alltreat/test.txt \
#       --checkpoint-dir robokop/CGGD_alltreat/models/conve/checkpoints \
#       --train-file robokop/CGGD_alltreat/train.txt \
#       --output-dir results/tracin \
#       --max-hops 3
# ============================================================================

# ============================================================================
# Utility Rules
# ============================================================================

rule clean:
    """
    Remove all generated files for the current style
    """
    params:
        base_dir = BASE_DIR
    shell:
        """
        rm -rf {params.base_dir}
        """

rule clean_models:
    """
    Remove only trained models (keep data preprocessing)
    """
    params:
        models_dir = f"{BASE_DIR}/models",
        eval_dir = f"{BASE_DIR}/results/evaluation",
        tracin_dir = f"{BASE_DIR}/results/tracin"
    shell:
        """
        rm -rf {params.models_dir}
        rm -rf {params.eval_dir}
        rm -rf {params.tracin_dir}
        """

rule clean_results:
    """
    Remove only analysis results (keep models and data)
    """
    params:
        eval_dir = f"{BASE_DIR}/results/evaluation",
        tracin_dir = f"{BASE_DIR}/results/tracin"
    shell:
        """
        rm -rf {params.eval_dir}
        rm -rf {params.tracin_dir}
        """

rule clean_all_styles:
    """
    Remove all robokop directories (all styles)
    """
    shell:
        """
        rm -rf robokop/
        """

# ============================================================================
# Info and Help
# ============================================================================

rule info:
    """
    Display pipeline information and memory optimization details
    """
    run:
        print("""
╔═════════════════════════════════════════════════════════════════════════════╗
║          ConvE PyTorch Pipeline - Memory-Optimized Implementation           ║
╚═════════════════════════════════════════════════════════════════════════════╝

This pipeline uses train_pytorch.py with memory-efficient loss computation:

KEY IMPROVEMENTS:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. EFFICIENT LOSS COMPUTATION
   • Avoids creating [batch_size × num_entities] label matrix
   • Memory reduction: 10,000-50,000x for label storage
   • Example: 48.83 MB → 1 KB for batch=256, entities=50k

2. LARGE BATCH SIZES
   • Default: 512 (increased from 256)
   • Possible: 1024+ with mixed precision
   • Benefits: Faster training, better convergence, efficient TracIn

3. MIXED PRECISION (FP16)
   • Enable: use_mixed_precision: true in config.yaml
   • Benefits: 2x memory reduction + 2x speed boost
   • Recommended for: V100, A100, RTX 20xx/30xx/40xx GPUs

4. TRACIN READY
   • Checkpoints saved automatically every N epochs
   • Compatible with run_tracin.py and batch_tracin_with_filtering.py
   • Large batch sizes → faster TracIn computation

CONFIGURATION:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Edit config.yaml to adjust:
  • batch_size: 512 (recommended) or 1024 (for large KGs)
  • use_mixed_precision: true (recommended for modern GPUs)
  • num_epochs: 100 (or more for large datasets)
  • checkpoint_frequency: 2 (save checkpoints every N epochs)

USAGE:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  # Run full pipeline
  snakemake --snakefile Snakefile_pytorch_conve --cores all

  # Dry run (see what will be executed)
  snakemake --snakefile Snakefile_pytorch_conve -n

  # Run specific rule
  snakemake --snakefile Snakefile_pytorch_conve train_model --cores all

  # Clean and restart
  snakemake --snakefile Snakefile_pytorch_conve clean
  snakemake --snakefile Snakefile_pytorch_conve --cores all

DOCUMENTATION:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

See MEMORY_OPTIMIZATION_GUIDE.md for detailed information about:
  • Root cause analysis of memory bottleneck
  • Mathematical derivation of efficient loss computation
  • Performance comparisons and benchmarks
  • TracIn integration and usage

╚═════════════════════════════════════════════════════════════════════════════╝
        """)
